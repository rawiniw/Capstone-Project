{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(map(pd.read_csv, \n",
    "                   ['walmart.csv', \n",
    "                    'target.csv',\n",
    "                    'amazon.csv',\n",
    "                    'costco.csv',\n",
    "                    'kroger.csv']),ignore_index = True)\n",
    "\n",
    "df.loc[:,'tokens_final'] = \\\n",
    "df.loc[:,'tokens_final'].apply(lambda x: literal_eval(x))\n",
    "\n",
    "def replace_in_list(lis, old, new):\n",
    "    for i in range(len(lis)):\n",
    "        if lis[i] == old:\n",
    "            lis[i] = new\n",
    "    return(lis)\n",
    "\n",
    "df['tokens_final'] = \\\n",
    "df['tokens_final'].map(lambda x: replace_in_list(x,\"employee\", \"associate\"))\n",
    "\n",
    "df['tokens_final'] = \\\n",
    "df['tokens_final'].map(lambda x: replace_in_list(x,\"guest\", \"customer\"))\n",
    "\n",
    "df['tokens_final'] = \\\n",
    "df['tokens_final'].map(lambda x: replace_in_list(x,\"consumer\", \"customer\"))\n",
    "\n",
    "remove_words =['performance','officer','value','award','amount',\n",
    "               'cash','chairman','vice','option','president',\n",
    "               'base','inc.','grant','voting','election','unit',\n",
    "               'audit','benefit','date','service','management',\n",
    "               'include','number','name','person','proposal',\n",
    "               'section','report','cost','receive','pension',\n",
    "               'rate','interest','fuel','rate','serve','sale','pension']\n",
    "\n",
    "df[\"tokens_final\"] = \\\n",
    "    df[\"tokens_final\"].map(lambda x: \\\n",
    "    [word for word in x if word.lower() not in remove_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>company</th>\n",
       "      <th>tokens_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>[street, website, www.walmartstores.com, notic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>[wal-mart, store, street, website, www.walmart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>[nonqualified, compensation, potential, paymen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>[compensation, store, compensation, amend, jan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>[nominee, statement, company, ratification, ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3710</th>\n",
       "      <td>2019</td>\n",
       "      <td>Kroger</td>\n",
       "      <td>[company, termination, cause, treatment, provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3711</th>\n",
       "      <td>2019</td>\n",
       "      <td>Kroger</td>\n",
       "      <td>[purpose, approve, company, provide, provision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>2019</td>\n",
       "      <td>Kroger</td>\n",
       "      <td>[security, restriction, represent, book, accou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>2019</td>\n",
       "      <td>Kroger</td>\n",
       "      <td>[case, jurisdiction, approve, discretion, prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>2019</td>\n",
       "      <td>Kroger</td>\n",
       "      <td>[company, extent, deems, discretion, purpose, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3715 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year  company                                       tokens_final\n",
       "0     2010  Walmart  [street, website, www.walmartstores.com, notic...\n",
       "1     2010  Walmart  [wal-mart, store, street, website, www.walmart...\n",
       "2     2010  Walmart  [nonqualified, compensation, potential, paymen...\n",
       "3     2010  Walmart  [compensation, store, compensation, amend, jan...\n",
       "4     2010  Walmart  [nominee, statement, company, ratification, ap...\n",
       "...    ...      ...                                                ...\n",
       "3710  2019   Kroger  [company, termination, cause, treatment, provi...\n",
       "3711  2019   Kroger  [purpose, approve, company, provide, provision...\n",
       "3712  2019   Kroger  [security, restriction, represent, book, accou...\n",
       "3713  2019   Kroger  [case, jurisdiction, approve, discretion, prov...\n",
       "3714  2019   Kroger  [company, extent, deems, discretion, purpose, ...\n",
       "\n",
       "[3715 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ran once to write files\n",
    "\n",
    "# import os\n",
    "# os.mkdir(\"temp_text\")\n",
    "\n",
    "# companies = df[\"company\"].drop_duplicates().values\n",
    "\n",
    "# for company in companies:\n",
    "#     os.mkdir(\"temp_text\\\\\" + company)\n",
    "    \n",
    "# for index, row in df.iterrows():\n",
    "#     company = row[\"company\"]\n",
    "#     year = row[\"year\"]\n",
    "#     text = ' '.join(row[\"tokens_final\"])\n",
    "    \n",
    "#     with open(f\"temp_text\\\\{company}\\\\{year}_{company}_{index}.txt\", 'w', encoding='utf8') as f:\n",
    "#         f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gensim\n",
    "\n",
    "def iter_documents(top_directory):\n",
    "    \"\"\"Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.\"\"\"\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for file in filter(lambda file: file.endswith('.txt'), files):\n",
    "            document = open(os.path.join(root, file), encoding='utf8').read() # read the entire document, as one big string\n",
    "            yield gensim.utils.tokenize(document, lower=True) # or whatever tokenization suits you\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, top_dir):\n",
    "        self.top_dir = top_dir\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_documents(top_dir))\n",
    "        self.dictionary.filter_extremes(no_below=1, keep_n=30000) # check API docs for pruning params\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in iter_documents(self.top_dir):\n",
    "            yield self.dictionary.doc2bow(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_corpus = MyCorpus('temp_text/walmart')\n",
    "amazon_corpus  = MyCorpus('temp_text/amazon')\n",
    "costco_corpus  = MyCorpus('temp_text/costco')\n",
    "target_corpus  = MyCorpus('temp_text/target')\n",
    "kroger_corpus  = MyCorpus('temp_text/kroger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "ntopic = 10\n",
    "\n",
    "lda_walmart = LdaModel(walmart_corpus, num_topics=ntopic, id2word=walmart_corpus.dictionary)\n",
    "lda_amazon  = LdaModel(amazon_corpus , num_topics=ntopic, id2word=amazon_corpus.dictionary)\n",
    "lda_costco  = LdaModel(costco_corpus , num_topics=ntopic, id2word=costco_corpus.dictionary)\n",
    "lda_target  = LdaModel(target_corpus , num_topics=ntopic, id2word=target_corpus.dictionary)\n",
    "lda_kroger  = LdaModel(kroger_corpus , num_topics=ntopic, id2word=kroger_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [(\"Walmart\", walmart_corpus, lda_walmart), \n",
    "         (\"Amazon\", amazon_corpus, lda_amazon),\n",
    "         (\"Costco\", costco_corpus, lda_costco), \n",
    "         (\"Target\", target_corpus, lda_target), \n",
    "         (\"Kroger\", kroger_corpus, lda_kroger)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "names_prod = list(itertools.product(names, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def average_topic(flat_list):\n",
    "\n",
    "    d = OrderedDict()\n",
    "    for prob, topic in flat_list:\n",
    "        d.setdefault(topic, []).append(prob)\n",
    "\n",
    "    d = [(sum(v) / len(v), k) for k, v in d.items()]\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('member', 0.020815318),\n",
      "    ('experience', 0.015948776),\n",
      "    ('cngc', 0.014865066),\n",
      "    ('business', 0.011187044),\n",
      "    ('incentive', 0.010436272),\n",
      "    ('corporation', 0.009863281),\n",
      "    ('review', 0.009304732),\n",
      "    ('governance', 0.009019247),\n",
      "    ('group', 0.008369606),\n",
      "    ('rule', 0.008257974)]\n",
      "\n",
      "[   ('column', 0.017674008),\n",
      "    ('equity', 0.0146302385),\n",
      "    ('incentive', 0.010269762),\n",
      "    ('payment', 0.009645595),\n",
      "    ('information', 0.00938826),\n",
      "    ('table', 0.008672465),\n",
      "    ('review', 0.0083433),\n",
      "    ('douglas', 0.008292766),\n",
      "    ('rule', 0.00827823),\n",
      "    ('market', 0.007920543)]\n",
      "\n",
      "[   ('transaction', 0.016856004),\n",
      "    ('member', 0.014321849),\n",
      "    ('review', 0.012210391),\n",
      "    ('policy', 0.011638354),\n",
      "    ('goal', 0.011564127),\n",
      "    ('incentive', 0.01017085),\n",
      "    ('cngc', 0.009424425),\n",
      "    ('accountant', 0.009326033),\n",
      "    ('recipient', 0.009256202),\n",
      "    ('target', 0.008195261)]\n",
      "\n",
      "[   ('transaction', 0.014363549),\n",
      "    ('business', 0.0126234),\n",
      "    ('material', 0.01241821),\n",
      "    ('associate', 0.00916168),\n",
      "    ('cngc', 0.008836774),\n",
      "    ('equity', 0.008449217),\n",
      "    ('defer', 0.0075979624),\n",
      "    ('rule', 0.0074918517),\n",
      "    ('time', 0.0074329074),\n",
      "    ('instruction', 0.0073637296)]\n",
      "\n",
      "[   ('defer', 0.020596625),\n",
      "    ('contribution', 0.0151858255),\n",
      "    ('equity', 0.012111226),\n",
      "    ('account', 0.011239773),\n",
      "    ('cmdc', 0.010454961),\n",
      "    ('governance', 0.009122334),\n",
      "    ('measure', 0.008379229),\n",
      "    ('incentive', 0.008206335),\n",
      "    ('program', 0.00796875),\n",
      "    ('participant', 0.007964951)]\n",
      "\n",
      "[   ('goal', 0.018108523),\n",
      "    ('incentive', 0.017790815),\n",
      "    ('program', 0.011521167),\n",
      "    ('target', 0.008758558),\n",
      "    ('result', 0.008614029),\n",
      "    ('cngc', 0.008110365),\n",
      "    ('restrict', 0.007969611),\n",
      "    ('payment', 0.007957374),\n",
      "    ('hold', 0.0075473674),\n",
      "    ('policy', 0.0074028987)]\n",
      "\n",
      "[   ('incentive', 0.017260043),\n",
      "    ('target', 0.0156668),\n",
      "    ('cngc', 0.013475682),\n",
      "    ('income', 0.012462263),\n",
      "    ('goal', 0.011397852),\n",
      "    ('payment', 0.009355652),\n",
      "    ('make', 0.007262844),\n",
      "    ('equity', 0.0072476),\n",
      "    ('period', 0.0069489065),\n",
      "    ('result', 0.0069342563)]\n",
      "\n",
      "[   ('target', 0.019368984),\n",
      "    ('incentive', 0.016363824),\n",
      "    ('goal', 0.015136152),\n",
      "    ('payment', 0.013675567),\n",
      "    ('equity', 0.013637951),\n",
      "    ('income', 0.012802228),\n",
      "    ('time', 0.012519759),\n",
      "    ('measure', 0.010892905),\n",
      "    ('show', 0.010300024),\n",
      "    ('period', 0.0084011415)]\n",
      "\n",
      "[   ('governance', 0.012236504),\n",
      "    ('risk', 0.011902815),\n",
      "    ('business', 0.011413673),\n",
      "    ('member', 0.010254936),\n",
      "    ('hold', 0.009466608),\n",
      "    ('policy', 0.008513626),\n",
      "    ('time', 0.008394788),\n",
      "    ('experience', 0.0077690547),\n",
      "    ('associate', 0.0074534426),\n",
      "    ('incentive', 0.0069486867)]\n",
      "\n",
      "[   ('incentive', 0.011310308),\n",
      "    ('material', 0.011265695),\n",
      "    ('governance', 0.009750038),\n",
      "    ('program', 0.009293055),\n",
      "    ('business', 0.009154368),\n",
      "    ('time', 0.007775401),\n",
      "    ('believe', 0.0071546305),\n",
      "    ('store', 0.007015252),\n",
      "    ('measure', 0.006869569),\n",
      "    ('member', 0.006488482)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(ntopic):\n",
    "    pp.pprint(lda_walmart.show_topic(topicid=i))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Amazon',\n",
       "  <__main__.MyCorpus at 0x175fb1a2f28>,\n",
       "  <gensim.models.ldamodel.LdaModel at 0x175fb21e390>),\n",
       " ('Walmart',\n",
       "  <__main__.MyCorpus at 0x175fb18d320>,\n",
       "  <gensim.models.ldamodel.LdaModel at 0x175faec3f28>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_prod[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart model applied to Walmart documents\n",
      "[   (0.5749485726530353, 0),\n",
      "    (0.659026976513886, 1),\n",
      "    (0.4697784031475229, 2),\n",
      "    (0.4411118731981045, 3),\n",
      "    (0.4724532369684601, 4),\n",
      "    (0.4844073406833259, 5),\n",
      "    (0.36388357516636355, 6),\n",
      "    (0.4208052073600156, 7),\n",
      "    (0.5742469740051969, 8),\n",
      "    (0.5099018941637113, 9)]\n",
      "\n",
      "Walmart model applied to Amazon documents\n",
      "[   (0.4584708307267499, 0),\n",
      "    (0.1604150372640744, 1),\n",
      "    (0.1915824999711637, 2),\n",
      "    (0.21484494816292735, 3),\n",
      "    (0.13429291695881057, 4),\n",
      "    (0.15634363830562623, 5),\n",
      "    (0.0771658681333065, 6),\n",
      "    (0.06407589491988931, 7),\n",
      "    (0.27291325400903743, 8),\n",
      "    (0.2841162194811444, 9)]\n",
      "\n",
      "Walmart model applied to Costco documents\n",
      "[   (0.46902829638984, 0),\n",
      "    (0.19138230993239969, 1),\n",
      "    (0.15899273125926056, 2),\n",
      "    (0.1698810890163869, 3),\n",
      "    (0.18826918323834738, 4),\n",
      "    (0.20826677218105438, 5),\n",
      "    (0.05865619461983442, 6),\n",
      "    (0.0804566657791535, 7),\n",
      "    (0.23010121727068172, 8),\n",
      "    (0.3017952565803015, 9)]\n",
      "\n",
      "Walmart model applied to Target documents\n",
      "[   (0.2814172803723967, 0),\n",
      "    (0.17623166629592402, 1),\n",
      "    (0.09393449138257313, 2),\n",
      "    (0.2610520904569482, 3),\n",
      "    (0.1540875974135449, 4),\n",
      "    (0.17153911900209606, 5),\n",
      "    (0.10079961297894592, 6),\n",
      "    (0.08165641306589047, 7),\n",
      "    (0.22100722337852957, 8),\n",
      "    (0.26069577280525275, 9)]\n",
      "\n",
      "Walmart model applied to Kroger documents\n",
      "[   (0.2605788649173274, 0),\n",
      "    (0.16442301933683695, 1),\n",
      "    (0.11511523746089151, 2),\n",
      "    (0.17177745998925537, 3),\n",
      "    (0.11316003515997429, 4),\n",
      "    (0.16323158666060839, 5),\n",
      "    (0.13047483241953478, 6),\n",
      "    (0.13110056075239832, 7),\n",
      "    (0.19149332568528324, 8),\n",
      "    (0.35710522398141037, 9)]\n",
      "\n",
      "Amazon model applied to Walmart documents\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2317 is out of bounds for axis 1 with size 2317",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ac48dee704f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{model_txt} model applied to {corpus_txt} documents\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-ac48dee704f9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{model_txt} model applied to {corpus_txt} documents\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chenson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mget_document_topics\u001b[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[0;32m   1327\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1330\u001b[0m         \u001b[0mtopic_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# normalize distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chenson\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mexpElogbetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2317 is out of bounds for axis 1 with size 2317"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "for index, pair in enumerate(names_prod):\n",
    "    model_txt  = pair[0][0]\n",
    "    corpus_txt = pair[1][0]\n",
    "    \n",
    "    model  = pair[0][2]\n",
    "    corpus = pair[1][1]\n",
    "    \n",
    "    print(f\"{model_txt} model applied to {corpus_txt} documents\")\n",
    "    tag = [model.get_document_topics(item) for item in corpus]\n",
    "    tag = [tup[::-1] for tup in flatten(tag)]\n",
    "    \n",
    "    topic_avg = average_topic(tag)\n",
    "    topic_avg = sorted(topic_avg, key = lambda x: x[1])\n",
    "    \n",
    "    pp.pprint(topic_avg)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
