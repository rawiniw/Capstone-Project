{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the code on Target data\n",
    "March 24th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pdftotext\n",
    "import re\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "\n",
    "def is_FactSet(filepath):\n",
    "    check_words = [\"Corrected Transcript\",\n",
    "                  \"www.callstreet.com\",\n",
    "                  \"FactSet CallStreet, LLC\"]\n",
    "    pdfFileObj = open(filepath,'rb')\n",
    "    pdf = pdftotext.PDF(pdfFileObj)\n",
    "    text = \"\\n\\n\".join(pdf)\n",
    "    if all([ check_word in text for check_word in check_words]):\n",
    "        return(\"FactSet\")\n",
    "    else:\n",
    "        return(\"plain\")\n",
    "    \n",
    "\n",
    "\n",
    "def get_fp_cp(fp):\n",
    "    # fp means front page: from first page to Corporate Participants\n",
    "    # get names of Corporate Participants\n",
    "    output = list()\n",
    "    for item in fp[fp.index(\"CORPORATE PARTICIPANTS\"):]:\n",
    "        if \"Walmart\" in item:\n",
    "            for x in item.split(\".\"):\n",
    "                output = output + x.split(\",\")\n",
    "            for x in item.split(\"-\"):\n",
    "                output = output + x.split(\"   \")\n",
    "            for x in item.split(\"-\"):\n",
    "                output = output + x.split(\".\")\n",
    "            for x in item.split(\"   \"):\n",
    "                output = output + x.split(\",\")\n",
    "        else: \n",
    "            output = output + item.split()\n",
    "    return([item.strip() for item in output if len(item) > 1]) \n",
    "\n",
    "\n",
    "\n",
    "def parse_FactSet_para(filepath, filename):\n",
    "    \"\"\" return a dataframe, each row is a paragraph spoken by one person\n",
    "    # columns include (1)paragraph and (2)date\n",
    "    \"\"\" \n",
    "    pdfFileObj = open(filepath,'rb')\n",
    "    pdf = pdftotext.PDF(pdfFileObj)\n",
    "    text = \"\\n\\n\".join(pdf)\n",
    "\n",
    "    tmp = text.split(\"........................................\\\n",
    "..............................................................\\\n",
    "..............................................................\\\n",
    "..................................................................................\")\n",
    "\n",
    "    # process words appeared in the front page and CORPORATE PARTICIPANTS section\n",
    "    # fp means front page: from first page to Corporate Participants\n",
    "    # fp contains a lot of info that repeat in the file\n",
    "    fp = [item.strip() for item in tmp[0].split(\"\\r\\n\") if len(item)>=1]\n",
    "\n",
    "    # \n",
    "    to_be_removed = [fp[0], fp[1], fp[3]] + \\\n",
    "                    fp[2].split() + \\\n",
    "                    [x for x in fp[5].split(\" \") if len(x) >= 1]\n",
    "\n",
    "    # sort words to be removed by len to prevent missing to remove longer words\n",
    "    to_be_removed_df = pd.DataFrame(set(to_be_removed))\n",
    "    to_be_removed_df[\"len\"] = to_be_removed_df[0].map(lambda x: len(x))\n",
    "    to_be_removed_df = to_be_removed_df.sort_values([\"len\"], ascending = False)\n",
    "\n",
    "    # remove words from fp\n",
    "    for item in to_be_removed_df[0]:\n",
    "        for i in range(1, len(tmp)):\n",
    "            tmp[i] = tmp[i].replace(item, '')\n",
    "\n",
    "    # remove \"\\r\\n\"\n",
    "    for i in range(1, len(tmp)):\n",
    "        tmp[i] = tmp[i].replace(\"\\r\\n\", ' ').lstrip(\", \")\n",
    "\n",
    "    # \n",
    "    tmp = pd.DataFrame(tmp[2:], columns=[\"paragraph\"])\n",
    "    \n",
    "    # add date\n",
    "    tmp[\"date\"] = pd.to_datetime(filename[:8])\n",
    "\n",
    "    return(tmp)\n",
    "\n",
    "\n",
    "def parse_plain_para(filepath, filename):\n",
    "    \"\"\" return a dataframe, each row is a paragraph spoken by one person\n",
    "    # columns include (1)paragraph and (2)date\n",
    "    \"\"\" \n",
    "    pdfFileObj = open(filepath,'rb')\n",
    "    pdf = pdftotext.PDF(pdfFileObj)\n",
    "    # combine pages\n",
    "    text = \"\\n\\n\".join(pdf)\n",
    "    # locate \"\\r\\n\" to locate starts of paragraphs later\n",
    "    text = re.sub(\"\\r\\n\", \" WRAPTEXT \",text)\n",
    "\n",
    "    # remove unuseful terms\n",
    "    ##### if got time, revise this part bc it takes too long\n",
    "    to_remove = [\" THOMSON REUTERS STREETEVENTS\", \" ©2018 Thomson\", \n",
    "              \" Client Id\", '\\uf0b7', \" ©2017 Thomson\", \" ©2016 Thomson\",\n",
    "               \"WAL-MART STORES, INC. COMPANY CONFERENCE PRESENTATION\",\n",
    "                \"WAL-MART STORES INC. COMPANY CONFERENCE PRESENTATION \",\n",
    "                \"Thomson Reuters\", \"Investment Community \"]\n",
    "    for item in to_remove:\n",
    "        text = re.sub(item, \" \",text)\n",
    "\n",
    "    # tokenize and tag POS\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "\n",
    "    # locate starts of paragraphs\n",
    "    locate = list()\n",
    "    for i in range(len(pos)-2):\n",
    "        punctuation = [\"•\", \"’\"]\n",
    "        if pos[i][0] == \"WRAPTEXT\" and pos[i+1][1] == \"NNP\" and pos[i+1][0] != \"•\" and pos[i+2][1] == \"NNP\"\\\n",
    "        and pos[i+2][0] not in punctuation:\n",
    "            locate.append((i, [pos[i+1], pos[i+2]]))\n",
    "        elif pos[i][0] == \"WRAPTEXT\" and pos[i+1][1] == \"NNP\" and pos[i+1][0] not in punctuation and pos[i+2][0] == \":\" :\n",
    "            locate.append((i, [pos[i+1], pos[i+2]]))\n",
    "\n",
    "    tmp = list()\n",
    "    for i in range(0,len(locate)-1):\n",
    "        start_loc = int(locate[i][0])\n",
    "        end_loc = int(locate[i+1][0])\n",
    "        para = tokens[start_loc:end_loc]\n",
    "        tmp.append(\" \".join(para))\n",
    "        \n",
    "    tmp = pd.DataFrame(tmp, columns=[\"paragraph\"])\n",
    "    tmp[\"date\"] = pd.to_datetime(filename[:8])\n",
    "    return(tmp)\n",
    "\n",
    "def consolidate_files(years):\n",
    "    \"\"\"\n",
    "        input is list of years\n",
    "        output is df, columns include \"filename\", \"filepath\", \"type\"\n",
    "    \"\"\"\n",
    "    files = pd.DataFrame()\n",
    "    for year in years:\n",
    "        path = os.getcwd() + '\\Transcripts' + \"\\\\\" + str(year)\n",
    "        # get file names\n",
    "        tmp = pd.DataFrame(os.listdir(path), columns=[\"filename\"])\n",
    "        # get file path for each file\n",
    "        tmp[\"filepath\"] = tmp[\"filename\"].map(lambda x: path + \"\\\\\" + x)\n",
    "        # check if it's a fancy(FACTSET) file\n",
    "        tmp[\"type\"] = tmp[\"filename\"].map(lambda x: is_FactSet(path + \"\\\\\" + x))\n",
    "        files = pd.concat([files, tmp])\n",
    "        \n",
    "    files = files.reset_index(drop=True)\n",
    "    return(files)\n",
    "\n",
    "def consolidate_files_others(company):\n",
    "    \"\"\"\n",
    "        input is the name of company, the name of folder\n",
    "        output is df, columns include \"filename\", \"filepath\", \"type\"\n",
    "    \"\"\"\n",
    "    # get path\n",
    "    path = os.getcwd() + '\\Transcripts Scraping' + \"\\\\\" + company\n",
    "    # get file names\n",
    "    tmp = pd.DataFrame(os.listdir(path), columns=[\"filename\"])\n",
    "    # get file path for each file\n",
    "    tmp[\"filepath\"] = tmp[\"filename\"].map(lambda x: path + \"\\\\\" + x)\n",
    "    # check if it's a fancy(FACTSET) file\n",
    "    tmp[\"type\"] = tmp[\"filename\"].map(lambda x: is_FactSet(path + \"\\\\\" + x))     \n",
    "\n",
    "    return(tmp)\n",
    "\n",
    "\n",
    "def filenames_to_para(files):\n",
    "    \"\"\"\n",
    "        input is df, columns include \"filename\", \"filepath\", \"type\"\n",
    "        output is df, columns include paragraph and date\n",
    "    \"\"\"\n",
    "    paragraphs = pd.DataFrame()\n",
    "    for i in files.index:\n",
    "        filepath = files.loc[i, \"filepath\"]\n",
    "        filename = files.loc[i, \"filename\"]\n",
    "        if files.loc[i, \"type\"] == \"plain\":\n",
    "            paragraphs = pd.concat([paragraphs, parse_plain_para(filepath, filename)], ignore_index=True)\n",
    "        elif files.loc[i, \"type\"] == \"FactSet\":\n",
    "            paragraphs = pd.concat([paragraphs, parse_FactSet_para(filepath, filename)], ignore_index=True)\n",
    "    return(paragraphs)\n",
    "\n",
    "\n",
    "\n",
    "def get_unique_words(para_tokens):\n",
    "    \"\"\"\n",
    "    input is a pd.Series of lists of POS-tagged words\n",
    "    \"\"\"\n",
    "    all_tokens = list(chain.from_iterable(para_tokens))\n",
    "    all_tokens = pd.Series(all_tokens)\n",
    "    tokens_count = all_tokens.value_counts()\n",
    "    return(tokens_count)\n",
    "\n",
    "    \n",
    "\n",
    "# collect all tagged tokens\n",
    "def get_all_wordsPos(token_pos, duplicate=False):\n",
    "    all_tokens = pd.Series(chain.from_iterable(token_pos))\n",
    "    if duplicate == True:\n",
    "        all_tokens = all_tokens.drop_duplicates()\n",
    "    d = {\"word\": all_tokens.map(lambda x: x[0]),\n",
    "         \"pos\": all_tokens.map(lambda x: x[1])}\n",
    "    return(pd.DataFrame(d))\n",
    "\n",
    "\n",
    "\n",
    "# this is for wordnet lemmatizer to recognize POS\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#from transcript_functions import *\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = consolidate_files_others('Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# parse into paragraphs\n",
    "df = filenames_to_para(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df[\"tokens\"] = df[\"paragraph\"].map(nltk.word_tokenize)\n",
    "\n",
    "# tag Part of Speech\n",
    "df[\"tokens_pos\"] = df[\"tokens\"].map(nltk.pos_tag)\n",
    "\n",
    "## to check distribution of POS tags\n",
    "# df_all_tokens = get_all_wordsPos(df[\"tokens_pos\"])\n",
    "\n",
    "# filter words with pos not in pos_remaining\n",
    "pos_remaining = [\"NN\", \"NNS\", \n",
    "                 \"VB\", \"VBD\", \"VBN\", \"VBP\", \"VBZ\"\n",
    "                ] # take out \"JJ\", \"JJR\", \"JJS\"\n",
    "df[\"tokens_pos\"] = df[\"tokens_pos\"].map(lambda x: [tup for tup in x if tup[1] in pos_remaining])\n",
    "\n",
    "# lemmatize with POS\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"tokens_clean\"] = df[\"tokens_pos\"].map(lambda x: [lemmatizer.lemmatize(tup[0], get_wordnet_pos(tup[1])) for tup in x ])\n",
    "\n",
    "# remove rows that have no content after lemmatization\n",
    "df = df[df[\"tokens_clean\"].map(lambda x: len(x)>=1)]\n",
    "\n",
    "# remove punctuation\n",
    "punctuation = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~...\"\n",
    "df[\"tokens_clean\"] = df[\"tokens_clean\"].map(lambda x: [word for word in x if word.lower() not in punctuation])\n",
    "\n",
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df[\"tokens_clean\"] = df[\"tokens_clean\"].map(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# remove some specific cases\n",
    "others = [\"'s\", \"'re\", \"'ve\", \"n't\", \"'ll\", \"–\", \"'m\"]\n",
    "df[\"tokens_clean\"] = df[\"tokens_clean\"].map(lambda x: [word for word in x if word.lower() not in others])\n",
    "\n",
    "# lower case\n",
    "df[\"tokens_clean\"] = df[\"tokens_clean\"].map(lambda x: [word.lower() for word in x])\n",
    "\n",
    "df = df[df[\"tokens_clean\"].map(lambda x: len(x)>=1)]\n",
    "\n",
    "# # remove short words\n",
    "# df[\"tokens_clean\"] = df[\"tokens_clean\"].map(lambda x: [word for word in x if len(word)>3])\n",
    "\n",
    "# # remove long words\n",
    "# df[\"tokens_clean\"] = df[\"tokens_clean\"].map(lambda x: [word for word in x if len(word)<16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing some words\n",
    "removing_words = [\n",
    "    \"get\", \"think\", \"thing\", \"know\", \"kind\", \"way\", \"look\", \"year\",\n",
    "     \"question\", \"guy\", \"thank\", \"quarter\", \"lot\", \"want\", \"herein\",\n",
    "    \"talk\", \"guess\", \"see\", \"say\", \"make\", \"go\", \"store\", \"customer\",\n",
    "    \"business\", \"time\", \"ph\", \"take\", \"bit\", \"work\", \"morning\", \n",
    "    \"company\", \"mean\", \"wraptext\", \"”\", '’'\n",
    "]\n",
    "df[\"tokens_clean\"] = df[\"tokens_clean\"].map(lambda x: [word for word in x if word.lower() not in removing_words])\n",
    "\n",
    "## filter out rows containing too few words/tokens\n",
    "df[\"len_clean\"] = df[\"tokens_clean\"].map(len)\n",
    "df = df[df[\"len_clean\"]>10]\n",
    "\n",
    "# removing disclaimer\n",
    "df = df[df[\"paragraph\"].map(lambda x: \"The information in the transcripts\" not in x)]\n",
    "df = df[df[\"paragraph\"].map(lambda x: \"disclaimer\" not in x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing some paragraphs\n",
    "removing_words = [\"www.streetevents.com\"]\n",
    "\n",
    "mask = df[\"tokens_clean\"].map(lambda x: removing_words[0] not in x)\n",
    "df = df[mask]\n",
    "\n",
    "# removing short paragraphs\n",
    "df = df[df.len_clean > 22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check omni's tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"tokens_pos\"] = df[\"tokens\"].map(nltk.pos_tag)\n",
    "# tokens_pos = df[\"tokens_pos\"]\n",
    "# key_word = \"omni\"\n",
    "# check = get_unique_words(tokens_pos)\n",
    "# check = pd.DataFrame(check)\n",
    "# check[\"pos\"] = check.index\n",
    "# check[check[\"pos\"].map(lambda x: key_word in x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at unique words to get a sense what needs to be delete\n",
    "# unique_words = get_unique_words(df[\"tokens_clean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 1  ====================\n",
      "term            3.91%\n",
      "content         3.49%\n",
      "help            1.50%\n",
      "launch          1.44%\n",
      "team            1.29%\n",
      "guidance        1.15%\n",
      "investment      1.15%\n",
      "job             1.12%\n",
      "number          1.12%\n",
      "point           1.12%\n",
      "topic: 2  ====================\n",
      "term            6.98%\n",
      "category        3.65%\n",
      "growth          3.15%\n",
      "selection       2.22%\n",
      "opportunity      1.96%\n",
      "investment      1.66%\n",
      "market          1.59%\n",
      "mention         1.55%\n",
      "add             1.54%\n",
      "content         1.33%\n",
      "topic: 3  ====================\n",
      "unit            4.09%\n",
      "growth          2.61%\n",
      "continue        2.60%\n",
      "fulfillment      2.22%\n",
      "center          1.71%\n",
      "service         1.70%\n",
      "number          1.70%\n",
      "cost            1.30%\n",
      "seller          1.26%\n",
      "half            1.22%\n",
      "topic: 4  ====================\n",
      "continue        4.06%\n",
      "price           2.63%\n",
      "add             1.64%\n",
      "feature         1.60%\n",
      "let             1.49%\n",
      "service         1.46%\n",
      "device          1.44%\n",
      "increase        1.37%\n",
      "engagement      1.28%\n",
      "benefit         1.22%\n",
      "topic: 5  ====================\n",
      "cost            8.05%\n",
      "transportation      4.44%\n",
      "shipping        4.23%\n",
      "capacity        3.75%\n",
      "start           2.28%\n",
      "logistics       1.81%\n",
      "delivery        1.74%\n",
      "network         1.51%\n",
      "step            1.51%\n",
      "partner         1.32%\n",
      "topic: 6  ====================\n",
      "growth          8.80%\n",
      "rate            3.38%\n",
      "cost            1.85%\n",
      "saw             1.83%\n",
      "area            1.83%\n",
      "investment      1.70%\n",
      "capital         1.66%\n",
      "infrastructure      1.64%\n",
      "revenue         1.49%\n",
      "number          1.42%\n",
      "topic: 7  ====================\n",
      "seller          2.18%\n",
      "advertising      1.96%\n",
      "product         1.87%\n",
      "program         1.86%\n",
      "selection       1.79%\n",
      "continue        1.65%\n",
      "benefit         1.23%\n",
      "price           1.22%\n",
      "brand           1.19%\n",
      "offer           1.17%\n",
      "topic: 8  ====================\n",
      "guidance        4.32%\n",
      "rate            1.83%\n",
      "expense         1.58%\n",
      "anticipate      1.38%\n",
      "remain          1.27%\n",
      "change          1.25%\n",
      "estimate        1.14%\n",
      "move            1.09%\n",
      "income          1.08%\n",
      "tax             1.07%\n",
      "topic: 9  ====================\n",
      "penetration      1.43%\n",
      "fulfillment      1.34%\n",
      "point           1.32%\n",
      "market          1.28%\n",
      "center          1.14%\n",
      "delivery        1.12%\n",
      "efficiency      1.11%\n",
      "cost            1.08%\n",
      "plan            1.05%\n",
      "mobile          1.05%\n",
      "topic: 10  ====================\n",
      "grocery         3.68%\n",
      "growth          2.12%\n",
      "grow            1.80%\n",
      "expand          1.45%\n",
      "city            1.40%\n",
      "product         1.29%\n",
      "continue        1.25%\n",
      "delivery        1.22%\n",
      "ability         1.19%\n",
      "foods           1.17%\n",
      "topic: 11  ====================\n",
      "result          6.86%\n",
      "call            5.52%\n",
      "today           5.35%\n",
      "measure         4.92%\n",
      "release         4.06%\n",
      "press           3.78%\n",
      "include         3.74%\n",
      "filing          3.26%\n",
      "factor          2.19%\n",
      "conference      2.16%\n",
      "topic: 12  ====================\n",
      "product         1.45%\n",
      "continue        1.42%\n",
      "service         1.19%\n",
      "enterprise      1.17%\n",
      "online          1.03%\n",
      "sale            0.91%\n",
      "feature         0.90%\n",
      "people          0.82%\n",
      "part            0.81%\n",
      "price           0.80%\n",
      "topic: 13  ====================\n",
      "growth          6.65%\n",
      "game            3.48%\n",
      "impact          3.18%\n",
      "margin          3.13%\n",
      "video           2.74%\n",
      "price           2.34%\n",
      "medium          2.04%\n",
      "part            1.78%\n",
      "term            1.75%\n",
      "rate            1.74%\n",
      "topic: 14  ====================\n",
      "growth          8.57%\n",
      "term            5.77%\n",
      "rate            2.66%\n",
      "basis           2.59%\n",
      "capacity        2.23%\n",
      "unit            1.90%\n",
      "add             1.78%\n",
      "guidance        1.65%\n",
      "saw             1.54%\n",
      "continue        1.50%\n",
      "topic: 15  ====================\n",
      "growth          2.98%\n",
      "rate            2.58%\n",
      "content         2.57%\n",
      "investment      2.42%\n",
      "revenue         2.38%\n",
      "service         2.05%\n",
      "continue        1.58%\n",
      "video           1.27%\n",
      "point           1.22%\n",
      "part            1.13%\n",
      "topic: 16  ====================\n",
      "holiday         3.05%\n",
      "point           2.03%\n",
      "impact          1.74%\n",
      "help            1.66%\n",
      "season          1.65%\n",
      "day             1.54%\n",
      "growth          1.43%\n",
      "give            1.43%\n",
      "advertising      1.37%\n",
      "center          1.36%\n",
      "topic: 17  ====================\n",
      "tax            12.64%\n",
      "state           7.68%\n",
      "sale            5.90%\n",
      "number          3.48%\n",
      "collect         2.70%\n",
      "geography       1.70%\n",
      "growth          1.38%\n",
      "give            1.25%\n",
      "offer           1.05%\n",
      "price           1.02%\n",
      "topic: 18  ====================\n",
      "term            5.88%\n",
      "something       5.12%\n",
      "cash            4.01%\n",
      "opportunity      3.24%\n",
      "flow            3.03%\n",
      "team            2.84%\n",
      "margin          2.81%\n",
      "focus           2.75%\n",
      "continue        2.59%\n",
      "number          2.33%\n",
      "topic: 19  ====================\n",
      "revenue        11.69%\n",
      "increase        5.01%\n",
      "grow            3.87%\n",
      "segment         3.76%\n",
      "exchange        3.39%\n",
      "compare         3.08%\n",
      "impact          2.53%\n",
      "cash            2.00%\n",
      "flow            1.95%\n",
      "unit            1.93%\n",
      "topic: 20  ====================\n",
      "income          4.84%\n",
      "increase        2.91%\n",
      "guidance        2.90%\n",
      "rate            2.77%\n",
      "exchange        2.59%\n",
      "expense         2.12%\n",
      "compensation      2.11%\n",
      "compare         1.92%\n",
      "loss            1.77%\n",
      "segment         1.56%\n"
     ]
    }
   ],
   "source": [
    "# prepare count vector as the input of LDA model\n",
    "df[\"corpus\"] = df[\"tokens_clean\"].map(lambda x: ', '.join(x))\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_data = count_vectorizer.fit_transform(df[\"corpus\"])\n",
    "\n",
    "number_topics = 20\n",
    "\n",
    "# fix random_state at 33\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1, random_state =33)\n",
    "lda.fit(count_data)\n",
    "       \n",
    "number_words = 12\n",
    "# print_topics(lda, count_vectorizer, number_words)\n",
    "\n",
    "words = count_vectorizer.get_feature_names()\n",
    "\n",
    "# print topics and key words\n",
    "for topic in range(0,number_topics):\n",
    "    topic_vec = lda.components_[topic]\n",
    "    topic_idx = topic_vec.argsort()[:-10 - 1:-1]\n",
    "    print(\"topic:\", topic + 1, \" ====================\")\n",
    "    for i in topic_idx:\n",
    "        print('{:<10} {:>10.2%}'.format(words[i], topic_vec[i]/topic_vec.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x214fa2344e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAFBCAYAAABElbosAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3debycZX338c+XRFBERCEqBCRUcYltVZoiPrUtLVpZ1Pj0cUGrWFwoLYharabu2tpia91aJKWCFjdUtDVKKmoruFQ0AVmMARtZI9tBWUSsEPg9f9x3YDyckzNJ5jo5OXzer9d5ZeZert91z0xmvnPdy6SqkCRJ0mhts6U7IEmSNBsZsiRJkhowZEmSJDVgyJIkSWrAkCVJktSAIUuSJKkBQ5a0hSVZmuRNM73tJA9NcnOSOf39M5K8dBRt9+39R5IXjaq9gXbvk+TzSW5M8ulRt78R/Xhrko9uqfoD/bg0yZO3dD+ke4K5W7oD0tYkyc0Dd7cHfgHc3t//k6r62Ma2WVVHbmJfLgUeDKzr+/B94GTghKq6Y2Pa7tt6aVV9ZQP9vBzYYVP6OkG9twIPr6oXDLR/0CjansCz6B6nnatqXaMa0yLJ/sBHq2r3Ld2XUUmyALgEuNfW/vxI4xmypI1QVXeGjGGCyTR4elV9Jcn9gd8F3gc8ATh8lEWSzN2KPwD3BH6wKf3fyrdb0hbm7kJpBJJsl+S9Sa7s/96bZLt+3v5J1iZ5fZLr+t01fzSw7oeT/PXA/cVJzk1yU5IfJjlwqvpVdWNVLQOeC7woya+ObzvJLkm+kOSGJD9J8vUk2yT5CPBQ4PP97sDXJlmQpJK8JMnlwH8NTBv8cvawJN/pd8V9LskDB7d53GN0aZIn99vzeuC5fb3z+vl37n7s+/XGJJcluTbJyX2QZKAfL0pyef+YvmGS5+VtwJsHar1kyLbv3O4J2pzwcezn7ZbkM0nGklyS5JjJnrMk+yX5776d8/pRqvXzHpjkQ/1r6fok/57kvsB/ALv123JzX2+bJEv618qPk3xq/fPQt/XCflt/PNnjNLDsdkne1T+u16Tb3Xyfft7qJE8bWHZu/9jvM8T2nJHkr5J8M8lPk3wpyS797K/1/97Qb9MTkzw8yZn96+q6JJ/cUL+lmcqQJY3GG4D9gMcBjwX2Bd44MP8hwC7AfOBFwAlJHjm+kST70u3y+wtgJ+B3gEuH7URVfQdYC/z2BLNf3c+bR7f77PXdKvVC4HK6UbEdqurvBtb5XeDRwFMnKXkY8GJgN7rdlu8foo9fBP4G+GRf77ETLPbH/d/vAb9Ct5vyn8Yt8yTgkcABwJuTPHqCWm8ZV+vEIdve0HZP+Dj2QevzwHl0z/MBwCuT3K2NJPOB04C/Bh4IvAb4TJJ5/SIfodsd/RjgQcB7qupnwEHAlf227FBVVwLHAM/s+7wbcD1wXF9nIXA88MJ+3s7AhnY1vhN4BN3r+OH9dry5n/cJ4HkDyz4VuK6qzhliewCeTzfC+iBg234Z6F7jADv12/Qt4K+ALwEP6Pv7jxvoszRjGbKk0fgj4O1VdW1VjQFvo/tgG/SmqvpFVZ1J94H0nAnaeQlwUlV9uaruqKofVdWFG9mXK+k+6Ma7DdgV2LOqbquqr9fUP1761qr6WVX9fJL5H6mq7/UB4E3Ac9IfGL+Z/gh4d1VdXFU3A38JHDpuFO1tVfXzqjqPLthMFNY2te0Nbfdkj+NvAvOq6u1VdWtVXQz8C3DoBG28AFheVcv75/nLwErg4CS70oWpI6vq+r7GmRvYnj8B3lBVa6vqF8BbgWf12/Ms4AtV9bV+3puAOyZqJEmAlwGvqqqfVNVP6QLq+v5/HHhGku37+8/vp21wewZKfKiqftA/pp+iC3KTuY1uN+9uVfW/VfWNDSwrzViGLGk0dgMuG7h/WT9tvev7IDLZ/PX2AH64mX2ZD/xkgul/D6wBvpTk4iRLhmjrio2YfxlwL7oRu8010eM5l27kaL2rB27fwvAH5Q/T9oa2e7LHcU+6XXk3rP+jG+V68ARt7Ak8e9yyT6ILb3sAP6mq64fcnj2BfxtoZzXdiRAP7rf1zm3pX4M/nqSdeXSjZ2cPtPXFfjpVtaZv++l90HoGd4WsDW3PehvzfL0WCPCdJKuSvHiqB0GaiTzwXRqNK+k+aFb19x/aT1vvAUnuOxC0Hgp8b4J2rgAetqmdSPKbdCHrbt/8+5GJVwOvTvIY4KtJVlTVfwKTjWhNNdK1x8Dth9KNQFwH/IzuA3t9v+bQf1gP2e76x3Ow7XXANWx4d9cwhml70v5N9jjSPXeXVNXeQ/ThCrpRwJeNn9GPZD0wyU5VdcP48pO09eKq+uYEbV1Ft9tz/f3t6XYZTuQ64OfAY6rqR5Mss36X4TbA9/vgtcHtGcLdtqmqrqYbVSPJk4CvJPnaQD1pq+BIljQanwDemGRef0Dvm4Hx10R6W5Jtk/w28DRgoms2nQgcnuSA/oDm+UkeNVXxJDv2ByWfQneK/wUTLPO0/oDiADfRjXasv/zENXTHJ22sFyRZ2H94vx04tapuB34A3DvJIUnuRXd82nYD610DLOiPY5rIJ4BXJdkryQ7cdVzVKM7026y2N/A4fge4Kcnr0l2ba06SX+2D73gfpRsRemq/3L3TnSywe1VdRXeA+weSPCDJvZKsP27pGmDn9Afq95YC70iyZ9+/eUkW9/NOBZ6W5ElJtqV7jiZ8zPvLfvwL8J4kD+rbmj/umLJTgD8A/pS7RrE2uD1DPKRjdLsw73z9JXn2wLrX0wWx2ydYV5rRDFnSaPw13TEo5wMXAOf009a7mu7D4krgY3TH29ztWKv+wPXDgfcANwJn8sujLuN9PslP6UYS3gC8m8kv37A38BXgZuBbwAeq6ox+3t/ShcQbkrxmkvUn8hHgw/323ZvuIGyq6kbgz4APAj+iG9kaPNtwfcD8cZJzJmj3pL7tr9FdQ+l/gZdvRL82ZHPbnvBx7MPl0+mONbqEbmTog8D9xzdQVVcAi+l2J47RPX9/wV3vyS+kGxW8ELgWeGW/3oV0IfHi/rnaje6yHcvodl/+FDiL7jIeVNUq4Ci6QHQV3Wvwl876HOd1dLtCz0pyU7+dd56g0QfAbwH/B/jkwPSptmdSVXUL8A7gm/027Ud3fNu3012Xbhnwiqq6ZKq2pJkmUx/3KmlzZBZeQFKSNDVHsiRJkhowZEmSJDXg7kJJkqQGHMmSJElqYEZeJ2uXXXapBQsWbOluSJIkTenss8++rqrmjZ8+I0PWggULWLly5ZbuhiRJ0pSSXDbRdHcXSpIkNWDIkiRJasCQJUmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWrAkCVJktTAjPztwg1ZsOS0TVrv0mMPGXFPJEmSJudIliRJUgNDhawkBya5KMmaJEsmmP+oJN9K8oskrxmYvkeSryZZnWRVkleMsvOSJEkz1ZS7C5PMAY4DngKsBVYkWVZV3x9Y7CfAMcAzx62+Dnh1VZ2T5H7A2Um+PG5dSZKkWWeYkax9gTVVdXFV3QqcAiweXKCqrq2qFcBt46ZfVVXn9Ld/CqwG5o+k55IkSTPYMCFrPnDFwP21bEJQSrIAeDzw7UnmH5FkZZKVY2NjG9u8JEnSjDLM2YWZYFptTJEkOwCfAV5ZVTdNtExVnQCcALBo0aKNar8Vz2SUJEmbapiRrLXAHgP3dweuHLZAknvRBayPVdVnN657kiRJW6dhQtYKYO8keyXZFjgUWDZM40kCnAisrqp3b3o3JUmSti5T7i6sqnVJjgZOB+YAJ1XVqiRH9vOXJnkIsBLYEbgjySuBhcCvAy8ELkhybt/k66tqeYNtkSRJmjGGuuJ7H4qWj5u2dOD21XS7Ecf7BhMf0yVJkjSrecV3SZKkBgxZkiRJDRiyJEmSGjBkSZIkNWDIkiRJasCQJUmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWrAkCVJktSAIUuSJKkBQ5YkSVIDhixJkqQGDFmSJEkNGLIkSZIaMGRJkiQ1YMiSJElqwJAlSZLUgCFLkiSpAUOWJElSA4YsSZKkBgxZkiRJDRiyJEmSGjBkSZIkNWDIkiRJasCQJUmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0MFbKSHJjkoiRrkiyZYP6jknwryS+SvGZj1pUkSZqNpgxZSeYAxwEHAQuB5yVZOG6xnwDHAO/ahHUlSZJmnWFGsvYF1lTVxVV1K3AKsHhwgaq6tqpWALdt7LqSJEmz0TAhaz5wxcD9tf20YQy9bpIjkqxMsnJsbGzI5iVJkmamYUJWJphWQ7Y/9LpVdUJVLaqqRfPmzRuyeUmSpJlpmJC1Fthj4P7uwJVDtr8560qSJG21hglZK4C9k+yVZFvgUGDZkO1vzrqSJElbrblTLVBV65IcDZwOzAFOqqpVSY7s5y9N8hBgJbAjcEeSVwILq+qmidZttTGSJEkzxZQhC6CqlgPLx01bOnD7arpdgUOtK0mSNNt5xXdJkqQGDFmSJEkNGLIkSZIaMGRJkiQ1YMiSJElqwJAlSZLUgCFLkiSpAUOWJElSA4YsSZKkBgxZkiRJDRiyJEmSGjBkSZIkNWDIkiRJasCQJUmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWrAkCVJktSAIUuSJKkBQ5YkSVIDhixJkqQG5m7pDuguC5actknrXXrsISPuiSRJ2lyOZEmSJDVgyJIkSWrAkCVJktSAIUuSJKkBQ5YkSVIDhixJkqQGDFmSJEkNDBWykhyY5KIka5IsmWB+kry/n39+kn0G5r0qyaok30vyiST3HuUGSJIkzURThqwkc4DjgIOAhcDzkiwct9hBwN793xHA8f2684FjgEVV9avAHODQkfVekiRphhpmJGtfYE1VXVxVtwKnAIvHLbMYOLk6ZwE7Jdm1nzcXuE+SucD2wJUj6rskSdKMNUzImg9cMXB/bT9tymWq6kfAu4DLgauAG6vqSxMVSXJEkpVJVo6NjQ3bf0mSpBlpmJCVCabVMMskeQDdKNdewG7AfZO8YKIiVXVCVS2qqkXz5s0boluSJEkz1zAhay2wx8D93bn7Lr/JlnkycElVjVXVbcBngf+z6d2VJEnaOswdYpkVwN5J9gJ+RHfg+vPHLbMMODrJKcAT6HYLXpXkcmC/JNsDPwcOAFaOrPfaLAuWnLZJ61167CEj7okkSbPPlCGrqtYlORo4ne7swJOqalWSI/v5S4HlwMHAGuAW4PB+3reTnAqcA6wDvguc0GJDJEmSZpJhRrKoquV0QWpw2tKB2wUcNcm6bwHeshl9lCRJ2up4xXdJkqQGDFmSJEkNGLIkSZIaMGRJkiQ1YMiSJElqwJAlSZLUgCFLkiSpAUOWJElSA4YsSZKkBgxZkiRJDRiyJEmSGjBkSZIkNWDIkiRJasCQJUmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWrAkCVJktSAIUuSJKkBQ5YkSVIDhixJkqQGDFmSJEkNGLIkSZIaMGRJkiQ1YMiSJElqwJAlSZLUgCFLkiSpAUOWJElSA3O3dAd0z7FgyWmbtN6lxx4y4p5IktTeUCNZSQ5MclGSNUmWTDA/Sd7fzz8/yT4D83ZKcmqSC5OsTvLEUW6AJEnSTDRlyEoyBzgOOAhYCDwvycJxix0E7N3/HQEcPzDvfcAXq+pRwGOB1SPotyRJ0ow2zEjWvsCaqrq4qm4FTgEWj1tmMXBydc4Cdkqya5Idgd8BTgSoqlur6oYR9l+SJGlGGiZkzQeuGLi/tp82zDK/AowBH0ry3SQfTHLfiYokOSLJyiQrx8bGht4ASZKkmWiYkJUJptWQy8wF9gGOr6rHAz8D7nZMF0BVnVBVi6pq0bx584boliRJ0sw1zNmFa4E9Bu7vDlw55DIFrK2qb/fTT2WSkCWN2nSfzTjb60mSNs4wI1krgL2T7JVkW+BQYNm4ZZYBh/VnGe4H3FhVV1XV1cAVSR7ZL3cA8P1RdV6SJGmmmnIkq6rWJTkaOB2YA5xUVauSHNnPXwosBw4G1gC3AIcPNPFy4GN9QLt43DxJkqRZaaiLkVbVcrogNTht6cDtAo6aZN1zgUWb0UdJkqStjj+rI0mS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWpgqIuRSpK/lShJG8eRLEmSpAYMWZIkSQ24u1DSjOTuSUlbO0eyJEmSGjBkSZIkNWDIkiRJasCQJUmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDXgz+pIEv6Mj6TRcyRLkiSpAUOWJElSA4YsSZKkBgxZkiRJDRiyJEmSGvDsQkmaZp7JKN0zOJIlSZLUgCFLkiSpAUOWJElSA4YsSZKkBoYKWUkOTHJRkjVJlkwwP0ne388/P8k+4+bPSfLdJF8YVcclSZJmsinPLkwyBzgOeAqwFliRZFlVfX9gsYOAvfu/JwDH9/+u9wpgNbDjiPotSRqSZzNKW8YwI1n7Amuq6uKquhU4BVg8bpnFwMnVOQvYKcmuAEl2Bw4BPjjCfkuSJM1ow4Ss+cAVA/fX9tOGXea9wGuBOzZUJMkRSVYmWTk2NjZEtyRJkmauYUJWJphWwyyT5GnAtVV19lRFquqEqlpUVYvmzZs3RLckSZJmrmFC1lpgj4H7uwNXDrnMbwHPSHIp3W7G30/y0U3urSRJ0lZimJC1Atg7yV5JtgUOBZaNW2YZcFh/luF+wI1VdVVV/WVV7V5VC/r1/quqXjDKDZAkSZqJpjy7sKrWJTkaOB2YA5xUVauSHNnPXwosBw4G1gC3AIe367IkSdLMN9QPRFfVcrogNTht6cDtAo6aoo0zgDM2uoeSJElbIa/4LkmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWrAkCVJktTAUFd8lyRpWAuWnLZJ61167CEj7om0ZTmSJUmS1IAhS5IkqQFDliRJUgOGLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWrAkCVJktSAIUuSJKkBQ5YkSVIDhixJkqQGDFmSJEkNGLIkSZIamLulOyBJ0uZYsOS0TVrv0mMPGXFPpF/mSJYkSVIDhixJkqQGDFmSJEkNGLIkSZIaMGRJkiQ1YMiSJElqwJAlSZLUgCFLkiSpAUOWJElSA0OFrCQHJrkoyZokSyaYnyTv7+efn2SffvoeSb6aZHWSVUleMeoNkCRJmommDFlJ5gDHAQcBC4HnJVk4brGDgL37vyOA4/vp64BXV9Wjgf2AoyZYV5IkadYZZiRrX2BNVV1cVbcCpwCLxy2zGDi5OmcBOyXZtaquqqpzAKrqp8BqYP4I+y9JkjQjDROy5gNXDNxfy92D0pTLJFkAPB749kRFkhyRZGWSlWNjY0N0S5IkaeYaJmRlgmm1Mcsk2QH4DPDKqrppoiJVdUJVLaqqRfPmzRuiW5IkSTPXMCFrLbDHwP3dgSuHXSbJvegC1seq6rOb3lVJkqStxzAhawWwd5K9kmwLHAosG7fMMuCw/izD/YAbq+qqJAFOBFZX1btH2nNJkqQZbO5UC1TVuiRHA6cDc4CTqmpVkiP7+UuB5cDBwBrgFuDwfvXfAl4IXJDk3H7a66tq+Wg3Q5IkaWaZMmQB9KFo+bhpSwduF3DUBOt9g4mP15IkSZrVvOK7JElSA0ONZEmSpM6CJadt0nqXHnvIiHuimc6RLEmSpAYMWZIkSQ0YsiRJkhowZEmSJDVgyJIkSWrAkCVJktSAIUuSJKkBQ5YkSVIDXoxUkqQZbLovfurFVkfHkSxJkqQGDFmSJEkNGLIkSZIaMGRJkiQ1YMiSJElqwLMLJUnSFjObz2Z0JEuSJKkBQ5YkSVIDhixJkqQGDFmSJEkNGLIkSZIa8OxCSZJ0jzGdZzM6kiVJktSAIUuSJKkBQ5YkSVIDhixJkqQGDFmSJEkNGLIkSZIaMGRJkiQ1YMiSJElqwJAlSZLUgCFLkiSpgaFCVpIDk1yUZE2SJRPMT5L39/PPT7LPsOtKkiTNRlOGrCRzgOOAg4CFwPOSLBy32EHA3v3fEcDxG7GuJEnSrDPMSNa+wJqquriqbgVOARaPW2YxcHJ1zgJ2SrLrkOtKkiTNOqmqDS+QPAs4sKpe2t9/IfCEqjp6YJkvAMdW1Tf6+/8JvA5YMNW6A20cQTcKBvBI4KJN2J5dgOs2Yb1NMZ21rGc9691z6s3mbbOe9WZrvT2rat74iXOHWDETTBufzCZbZph1u4lVJwAnDNGfSSVZWVWLNqeNmVjLetaz3j2n3mzeNutZ755Wb5iQtRbYY+D+7sCVQy6z7RDrSpIkzTrDHJO1Atg7yV5JtgUOBZaNW2YZcFh/luF+wI1VddWQ60qSJM06U45kVdW6JEcDpwNzgJOqalWSI/v5S4HlwMHAGuAW4PANrdtkSzqbtbtxBteynvWsd8+pN5u3zXrWu0fVm/LAd0mSJG08r/guSZLUgCFLkiSpAUOWJElSA4asISQ5JskeUy850pr7JvnN/vbCJH+e5ODp7ENLSR6V5IAkO4ybfuCW6tOoJHlCkh372/dJ8rYkn0/yziT339L9G7UkJ2/pPkgzQZIn9e/Vf9CwxsOSvCbJ+5L8Q5IjZ+P7SmtJtk1yWJIn9/efn+SfkhyV5F4jq+OB71NLciPwM+CHwCeAT1fVWMN6b6H7vce5wJeBJwBnAE8GTq+qd7SqPR2SHAMcBawGHge8oqo+1887p6r22dD6M12SVcBj+7NrT6A74/ZU4IB++h9u0Q5uhiTjL8ES4PeA/wKoqmdMc38Or6oPTWdNab0k36mqffvbL6N7X/s34A+Az1fVsSOudwzwdOBMujP6zwWuB/4v8GdVdcYo681mST5G9xm7PXADsAPwWbr36VTVi0ZSZzaFrCQPqqprG7T7XeA36ELOc4FnAGfTBa7PVtVPR1zvArrwsR1wNbB7Vd2U5D7At6vq10dZb7r12/fEqro5yQK6APKRqnpfku9W1eOnoQ87V9WPG7W9uqoe3d/+pdCY5NyqelyLutMhyTnA94EPctevOnyC7hp4VNWZ09yfy6vqoSNs7/7AXwLPBNb/RMa1wOfofjrshlHV0tZv8P0qyQrg4KoaS3Jf4Kyq+rUR17sAeFxV3Z5ke2B5Ve2f5KHA56bjvXO2SHJ+Vf16krnAj4Dd+sc1wHmj+pzdancXJnnguL+dge8keUCSB464XFXVHVX1pap6CbAb8AHgQODiEdcCWFdVt1fVLcAPq+qmvhM/B+4YdbHBXXRJ7p/kxCTnJ/l4kgePuh4wp6puBqiqS4H9gYOSvJuJf4ppsyQ5Nsku/e1FSS4Gvp3ksiS/O+p6wPeSHN7fPi/Jor72I4DbGtSbVJL/GHGTi+i+YLyB7qLDZwA/r6ozWwWs/rU40d8FwKhfn5+iGxnYv6p2rqqd6Ubqrgc+PeJaACTZIcnbk6xKcmOSsSRnJfnjBrUekuT4JMcl2TnJW5NckORTSXZtUG9Rkq8m+WiSPZJ8ud/GFUlGHgiS7Jjkb5N8JMnzx837wKjrAdv0nzk70w1ajAFU1c+AdQ3qwV3Xt9wOuF9f73JgZLu41us/D45NcmGSH/d/q/tpOzWod06SNyZ52KjbnsA26S6Sfj+60az1u1y3Y4SP5TA/qzNTXQdcNm7afOAcum/YvzLCWr/0wV9Vt9FduX5ZP7o0arcm2b4PWb9xZye6b9kjD1nA3wBf7G//A3AV3ZD0HwL/TPetfpSuTvK4qjoXoB/RehpwEjDSb369Q6pqSX/774HnVtWKPvR8nC44jNJLgfcleSPd6/RbSa4ArujnjVSSyXavhm5EdGSq6g7gPUk+3f97De3fRx4MPJUu6AwK8N8jrrWgqt45OKGqrgbemeTFI6613sfodjE9FXgOcF/gFOCNSR5RVa8fYa0PA6f1Nb7a1z4EWAws7f8dpQ8AbwF2onuuXlVVT0lyQD/viSOu9yHgf4DPAC9O8v+A51fVL4D9RlwLug/ms+lei5XkIVV1dbpjTUf+hZFuBHlFkrOA3wHeCZBkHvCTBvU+RXcowP79/wOSPAR4Ed2XjqeMuN4D6F4rX01yNd0o+SerqsXP8Z0IXEh3ofQ3AJ/uv4DvR/f/bzSqaqv8A15DFwx+bWDaJY1qPWKat227SabvMri9I6x3zsDtc8fNO7dBvd2Bh0wy77ca1LsQmNvfPmvcvAsaPo/3Ax5LF5Qf3LDO7XRvhF+d4O/nrer2tQ8B/qZxjROBJ00y7+MjrvUl4LWDzxddyHsd8JVG23feuPsr+n+3AS4cca3vDty+fNy8Fv/XN1Tvuw3qjX//egPwTWDnwfe51n90IyN7NWr7McCzgEdNw3ZctCnzNqPe4GfRb9MF8av797IjGtTbjW43IXTh7lnAvqOssdWOZFXVu5KcQvdt+gq6b0tNDjCrqh+0aHcD9X4xyfTr6EZGRu1BSf6c7pvXjklS/auOBruUq2rtBuZ9c9T1gOOA5UmOBb6Y5L3cdYDjuQ3qAVDdsXrntWp/wGrgT6rqf8bP6P9vNFNVp9GNjLSs8ZINzHv+ZPM20XOBJcCZ/a7yAq6hG7l+zohrrfezJE+qqm8keTr9iERV3dEfHzJKg/+fx58VOmfEtQD+N92ZdvenG+l5ZlX9e7+b/vYG9bZLsk11I65U1TuSrAW+Rndg87Sobi/EJY3aXgW0/Hm6QZcleS3wr1V1DUD//+KP6Ubmm6mqrwNfT/JyuhGz5zLin7ypgRGy6o63PHWU7cPWvbtw/Yf1s/s3pi/TfXvQxvsX+n37wL/SjZiN9cPCzULIdKmqf+yP3/lT4BF0r/tHAP8O/NWW7NuIvJXJw/DLp7EfW72quj7Jh+jeT86q/thBuPPYxS9OuvKmOxL4YL/7+nvAi/t68+i+IIzS55LsUFU3V9Ub109M8nDgohHXgsidQTwAAAFHSURBVG7b/o7uMIenAn+a5MN0Bxq/rEG9zwO/D3xl/YSq+td+t/Y/Nqg32w1+6XhQP239l45nN6h3twGNqrqd7v9di/97zc2aswv7Y6MeVlXfi6d1j8xsfyzdPg3KDLu8yHQ+f9P9Wpnt9WY7n7/hzJqQNSgjPq37nmy2P5ZunwZlBlxeZFx/pu35m+7XymyvN9v5/A1nq91dmOT8yWYx+tO6Z7XZ/li6fdoIv3R5kST7A6cm2ZM2Z4tN6/M33a+V2V5vtvP523xbbchiek/rnu1m+2Pp9mlY0315EZje52+6Xyuzvd5s5/O3mbbmkPUFYIf1b4aDkpwx/d3Zqs32x9Lt07AOY9xFJKtqHXBYkn9uVHM6n7/pfq3M9nqznc/fZpqVx2RJkiRtaVvtz+pIkiTNZIYsSZKkBgxZkiRJDRiyJEmSGvj/sCc1h8JI1ZYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# assign topic to paragraphs\n",
    "topic_match = lda.transform(count_data)\n",
    "df[\"topic\"] = [ x.argsort()[-1] + 1 for x in topic_match]\n",
    "\n",
    "# # chech how many topics in each paragraphs\n",
    "# df[\"number of topics\"] = [sum(x>x.mean()) for x in topic_match]\n",
    "\n",
    "topic_counts = df[\"topic\"].value_counts()\n",
    "(topic_counts/topic_counts.sum()).plot(x=\"Topics\", y=\"percentage\",\n",
    "                                       title=\"Topic Distribution for selected events\",\n",
    "                                       kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize paragraph counts for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['quarter'] = pd.PeriodIndex(df.date, freq='Q')\n",
    "df['year'] = pd.PeriodIndex(df.date, freq='Y')\n",
    "df[\"year\"].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.crosstab(test[\"year\"], test[\"topic\"], normalize=\"index\")\n",
    "years = [ str(x) for x in tmp.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for topic in tmp:\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(years, tmp[topic].values)\n",
    "    plt.ylim(0, 0.4)\n",
    "    plt.xticks(years, years, rotation='vertical')\n",
    "    plt.title(\"topic \"+ str(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = test[test[\"topic\"]==18]\n",
    "check[check[\"year\"] == 2017][\"paragraph\"].iloc[0].replace(\"WRAPTEXT\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## search for certain key word\n",
    "keyword = \"shareholder\"\n",
    "check[check[\"paragraph\"].map(lambda x: keyword in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize components for each year (stop using for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_tp = df.groupby(by=[\"year\",\"topic\"])[\"topic\"].count().unstack().fillna(0)\n",
    "# tmp = event_tp.T.apply(sum)\n",
    "# event_tp = event_tp.apply(lambda x: x/tmp)\n",
    "# event_tp.index = [str(x)[:10] for x in event_tp.index]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16, 5))\n",
    "# colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', \n",
    "#           'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "# bottom = np.array([0]*event_tp.shape[0])\n",
    "# x = event_tp.index.values\n",
    "\n",
    "# for i in event_tp.columns:\n",
    "# #     print(event_tp[i].values)\n",
    "#     plt.bar(x, event_tp[i].values, width=0.5, color=colors[int(i-1)], bottom=bottom)\n",
    "#     bottom = bottom + event_tp[i]\n",
    "\n",
    "# plt.legend(event_tp.columns)\n",
    "# plt.title(\"Topic distribution across events\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
