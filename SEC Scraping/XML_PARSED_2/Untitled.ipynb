{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gensim\n",
    "\n",
    "def iter_documents(top_directory):\n",
    "    \"\"\"Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.\"\"\"\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for file in filter(lambda file: file.endswith('.txt'), files):\n",
    "            document = open(os.path.join(root, file), encoding='utf8').read() # read the entire document, as one big string\n",
    "            yield gensim.utils.tokenize(document, lower=True) # or whatever tokenization suits you\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, top_dir):\n",
    "        self.top_dir = top_dir\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_documents(top_dir))\n",
    "        self.dictionary.filter_extremes(no_below=1, keep_n=30000) # check API docs for pruning params\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in iter_documents(self.top_dir):\n",
    "            yield self.dictionary.doc2bow(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_corpus = MyCorpus('WMT_XML')\n",
    "amazon_corpus  = MyCorpus('AMZN_XML')\n",
    "costco_corpus  = MyCorpus('COST_XML')\n",
    "target_corpus  = MyCorpus('TGT_XML')\n",
    "kroger_corpus  = MyCorpus('KR_XML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "ntopic = 20\n",
    "\n",
    "lda_walmart = LdaModel(walmart_corpus, num_topics=ntopic, id2word=walmart_corpus.dictionary)\n",
    "lda_amazon  = LdaModel(amazon_corpus , num_topics=ntopic, id2word=amazon_corpus.dictionary)\n",
    "lda_costco  = LdaModel(costco_corpus , num_topics=ntopic, id2word=costco_corpus.dictionary)\n",
    "lda_target  = LdaModel(target_corpus , num_topics=ntopic, id2word=target_corpus.dictionary)\n",
    "lda_kroger  = LdaModel(kroger_corpus , num_topics=ntopic, id2word=kroger_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [(\"Walmart\", walmart_corpus, lda_walmart), \n",
    "         (\"Amazon\", amazon_corpus, lda_amazon),\n",
    "         (\"Costco\", costco_corpus, lda_costco), \n",
    "         (\"Target\", target_corpus, lda_target), \n",
    "         (\"Kroger\", kroger_corpus, lda_kroger)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "names_prod = list(itertools.product(names, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def average_topic(flat_list):\n",
    "\n",
    "    d = OrderedDict()\n",
    "    for prob, topic in flat_list:\n",
    "        d.setdefault(topic, []).append(prob)\n",
    "\n",
    "    d = [(sum(v) / len(v), k) for k, v in d.items()]\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('po', 0.010568354), ('overflow', 0.010198054), ('proxy', 0.009976957), ('middle', 0.009881891), ('director', 0.009625579), ('meeting', 0.0087029645), ('publisher', 0.008557172), ('incentive', 0.00793756), ('role', 0.007853022), ('hidden', 0.0077880383)]\n",
      "[('middle', 0.02719071), ('hidden', 0.026834177), ('overflow', 0.02457924), ('role', 0.013327985), ('solid', 0.012283826), ('publisher', 0.012236), ('fae', 0.012065034), ('topic', 0.011734632), ('codification', 0.011656206), ('meeting', 0.009445529)]\n",
      "[('publisher', 0.026780529), ('role', 0.025339868), ('overflow', 0.024932716), ('solid', 0.023256136), ('hidden', 0.022070892), ('middle', 0.019262971), ('codification', 0.014741223), ('topic', 0.012385511), ('double', 0.01102255), ('prefix', 0.010890941)]\n",
      "[('hidden', 0.019747863), ('overflow', 0.01911846), ('solid', 0.016834969), ('span', 0.016690116), ('indenture', 0.015170936), ('role', 0.014171551), ('middle', 0.0135618225), ('codification', 0.013368087), ('publisher', 0.009387346), ('topic', 0.008618733)]\n",
      "[('role', 0.02692475), ('publisher', 0.020572536), ('hidden', 0.02046977), ('fae', 0.019268602), ('overflow', 0.019078173), ('codification', 0.014733048), ('middle', 0.014343387), ('solid', 0.012091632), ('topic', 0.01173111), ('vat', 0.0100223115)]\n",
      "[('span', 0.1652483), ('role', 0.041124422), ('hidden', 0.031849716), ('overflow', 0.029563125), ('middle', 0.02812497), ('publisher', 0.026429467), ('solid', 0.022163391), ('codification', 0.017581869), ('topic', 0.01744386), ('ref', 0.013265292)]\n",
      "[('solid', 0.03713766), ('hidden', 0.035922747), ('overflow', 0.03147259), ('role', 0.029210739), ('codification', 0.024547873), ('middle', 0.024019577), ('span', 0.020149132), ('topic', 0.016586), ('publisher', 0.015835939), ('subparagraph', 0.011611565)]\n",
      "[('prospectus', 0.020576496), ('publisher', 0.01913653), ('role', 0.019043094), ('indenture', 0.017654015), ('solid', 0.012027184), ('overflow', 0.011898849), ('middle', 0.011802398), ('codification', 0.011043587), ('hidden', 0.010200201), ('fae', 0.009511931)]\n",
      "[('overflow', 0.046834327), ('hidden', 0.04549604), ('middle', 0.044773065), ('solid', 0.040853728), ('role', 0.024950467), ('publisher', 0.023625264), ('codification', 0.022748612), ('topic', 0.018370975), ('double', 0.0143453255), ('fae', 0.013561425)]\n",
      "[('publisher', 0.03926015), ('role', 0.0386105), ('overflow', 0.028389247), ('codification', 0.023643984), ('topic', 0.021186855), ('middle', 0.020018293), ('solid', 0.018912746), ('hidden', 0.01753412), ('fae', 0.015778698), ('prefix', 0.015767792)]\n",
      "[('role', 0.033970077), ('publisher', 0.026759919), ('codification', 0.023440547), ('middle', 0.022937607), ('overflow', 0.02177569), ('hidden', 0.02141701), ('solid', 0.019679116), ('topic', 0.018314345), ('fae', 0.017201703), ('prefix', 0.013522567)]\n",
      "[('middle', 0.02178905), ('fae', 0.018872106), ('hidden', 0.016490271), ('overflow', 0.015821932), ('indenture', 0.013658915), ('solid', 0.01273275), ('role', 0.012601494), ('associate', 0.012458461), ('publisher', 0.009284874), ('topic', 0.009104116)]\n",
      "[('hidden', 0.026606316), ('overflow', 0.026072599), ('middle', 0.019843021), ('role', 0.017398812), ('fae', 0.016348105), ('solid', 0.014050224), ('publisher', 0.014003958), ('codification', 0.011601524), ('vat', 0.008977924), ('topic', 0.008781768)]\n",
      "[('overflow', 0.030800208), ('hidden', 0.025994947), ('publisher', 0.020007703), ('middle', 0.01939236), ('role', 0.0184218), ('solid', 0.017663637), ('codification', 0.010681468), ('topic', 0.009914485), ('participant', 0.009210052), ('prefix', 0.008989255)]\n",
      "[('middle', 0.023568071), ('hidden', 0.020069644), ('role', 0.019972136), ('overflow', 0.019895466), ('prospectus', 0.018966744), ('solid', 0.017451655), ('indenture', 0.0155013595), ('span', 0.014171872), ('publisher', 0.013887868), ('topic', 0.011242167)]\n",
      "[('prospectus', 0.040135358), ('supplement', 0.018036284), ('role', 0.015837902), ('solid', 0.015325131), ('publisher', 0.015149738), ('trustee', 0.011923552), ('indenture', 0.011761655), ('redemption', 0.009393513), ('contents', 0.007261946), ('hidden', 0.00702572)]\n",
      "[('middle', 0.026274534), ('overflow', 0.024984177), ('hidden', 0.021712817), ('prospectus', 0.019114183), ('solid', 0.018501304), ('publisher', 0.014953807), ('role', 0.014847522), ('topic', 0.012277422), ('span', 0.012107526), ('codification', 0.011667508)]\n",
      "[('hidden', 0.047682554), ('overflow', 0.038026653), ('middle', 0.03249748), ('role', 0.026055232), ('topic', 0.023398498), ('solid', 0.02243172), ('publisher', 0.019385153), ('fae', 0.018986324), ('codification', 0.017796159), ('prefix', 0.01598006)]\n",
      "[('hidden', 0.03921304), ('middle', 0.038179867), ('role', 0.0344344), ('overflow', 0.031912953), ('publisher', 0.02405764), ('codification', 0.018784963), ('solid', 0.017984558), ('fae', 0.016754523), ('prefix', 0.014937873), ('topic', 0.010673668)]\n",
      "[('hidden', 0.04223227), ('overflow', 0.04053558), ('middle', 0.029582847), ('solid', 0.021486972), ('role', 0.020051582), ('codification', 0.017081512), ('prefix', 0.016866265), ('publisher', 0.016557166), ('topic', 0.01352685), ('bold', 0.012058596)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(lda_walmart.show_topic(topicid=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart model applied to Walmart documents\n",
      "[(0.5351285361469938, 0), (0.3512907636097886, 1), (0.2870668436993252, 2), (0.6492803201852542, 3), (0.43556032329797745, 4), (0.38744891180910845, 5), (0.1578330531483516, 6), (0.275230953656137, 7), (0.5797262836633057, 8), (0.3702181575113329, 9), (0.18750847125839856, 10), (0.4245526646781299, 11), (0.4487048659939319, 12), (0.40819737014289087, 13), (0.7536786735057831, 14), (0.5062841631401822, 15), (0.3428046562605434, 16), (0.09892254579989683, 17), (0.561461015811397, 18), (0.4102288049590938, 19)]\n",
      "\n",
      "Walmart model applied to Amazon documents\n",
      "[(0.2984454673786576, 0), (0.1552486481765906, 1), (0.06550457083027471, 2), (0.12270808423107321, 3), (0.07920647785067558, 6), (0.03276656794228724, 7), (0.18427568819310705, 8), (0.043299071863293646, 9), (0.049793027341365814, 10), (0.05468397711714109, 11), (0.048771288358803955, 12), (0.025112731692691643, 13), (0.36130399197122476, 15), (0.026213195830307626, 16), (0.14118828591616714, 18), (0.01788010261952877, 19)]\n",
      "\n",
      "Walmart model applied to Costco documents\n",
      "[(0.3541301972203955, 0), (0.1703810031629271, 1), (0.1140048789924809, 2), (0.1026165110661703, 3), (0.0581791036000306, 6), (0.03638621645846537, 7), (0.15040412806690887, 8), (0.07200211818729128, 9), (0.045477365329861644, 10), (0.037438989701596176, 11), (0.09365646741711177, 12), (0.04323804581707174, 13), (0.5348256054955224, 15), (0.5825682282447815, 16), (0.04317044960334897, 18)]\n",
      "\n",
      "Walmart model applied to Target documents\n",
      "[(0.2836457560899564, 0), (0.10336836033891286, 1), (0.0869976612334342, 2), (0.22972266674465078, 3), (0.06298900814726949, 6), (0.04620258463546634, 7), (0.24141250467644287, 8), (0.010148636996746063, 9), (0.17167047712774502, 10), (0.134908076511188, 11), (0.1494733408482662, 12), (0.04212695732712746, 13), (0.06209159043750593, 14), (0.26466097876193445, 15), (0.0964593364901486, 16), (0.024811198003590107, 17), (0.3313736453918474, 18), (0.06146776513196528, 19)]\n",
      "\n",
      "Walmart model applied to Kroger documents\n",
      "[(0.31153435463531187, 0), (0.20288788705048236, 1), (0.13746489809396176, 2), (0.35501613995126846, 3), (0.06130436062812805, 6), (0.09962791949510574, 7), (0.12318443245744917, 8), (0.04389861617879621, 9), (0.035149999618254324, 10), (0.07064710295727027, 11), (0.06260698694735765, 12), (0.12377089262008667, 14), (0.2949586851047535, 15), (0.08353377744141552, 16), (0.06830281594635121, 18), (0.13084402959793806, 19)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "for index, pair in enumerate(names_prod[:5]):\n",
    "    model_txt  = pair[0][0]\n",
    "    corpus_txt = pair[1][0]\n",
    "    \n",
    "    model  = pair[0][2]\n",
    "    corpus = pair[1][1]\n",
    "    \n",
    "    print(f\"{model_txt} model applied to {corpus_txt} documents\")\n",
    "    tag = [model.get_document_topics(item) for item in corpus]\n",
    "    tag = [tup[::-1] for tup in flatten(tag)]\n",
    "    \n",
    "    topic_avg = average_topic(tag)\n",
    "    topic_avg = sorted(topic_avg, key = lambda x: x[1])\n",
    "    \n",
    "    print(topic_avg)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
