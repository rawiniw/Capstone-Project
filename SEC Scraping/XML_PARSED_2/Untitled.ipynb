{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gensim\n",
    "\n",
    "def iter_documents(top_directory):\n",
    "    \"\"\"Iterate over all documents, yielding a document (=list of utf8 tokens) at a time.\"\"\"\n",
    "    for root, dirs, files in os.walk(top_directory):\n",
    "        for file in filter(lambda file: file.endswith('.txt'), files):\n",
    "            document = open(os.path.join(root, file), encoding='utf8').read() # read the entire document, as one big string\n",
    "            yield gensim.utils.tokenize(document, lower=True) # or whatever tokenization suits you\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, top_dir):\n",
    "        self.top_dir = top_dir\n",
    "        self.dictionary = gensim.corpora.Dictionary(iter_documents(top_dir))\n",
    "        self.dictionary.filter_extremes(no_below=1, keep_n=30000) # check API docs for pruning params\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in iter_documents(self.top_dir):\n",
    "            yield self.dictionary.doc2bow(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_corpus = MyCorpus('WMT_XML')\n",
    "amazon_corpus  = MyCorpus('AMZN_XML')\n",
    "costco_corpus  = MyCorpus('COST_XML')\n",
    "target_corpus  = MyCorpus('TGT_XML')\n",
    "kroger_corpus  = MyCorpus('KR_XML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "lda_walmart = LdaModel(walmart_corpus, num_topics=10, id2word=walmart_corpus.dictionary)\n",
    "lda_amazon  = LdaModel(amazon_corpus , num_topics=10, id2word=amazon_corpus.dictionary)\n",
    "lda_costco  = LdaModel(costco_corpus , num_topics=10, id2word=costco_corpus.dictionary)\n",
    "lda_target  = LdaModel(target_corpus , num_topics=10, id2word=target_corpus.dictionary)\n",
    "lda_kroger  = LdaModel(kroger_corpus , num_topics=10, id2word=kroger_corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [(\"Walmart\", walmart_corpus, lda_walmart), \n",
    "         (\"Amazon\", amazon_corpus, lda_amazon),\n",
    "         (\"Costco\", costco_corpus, lda_costco), \n",
    "         (\"Target\", target_corpus, lda_target), \n",
    "         (\"Kroger\", kroger_corpus, lda_kroger)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "names_prod = list(itertools.product(names, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def average_topic(flat_list):\n",
    "\n",
    "    d = OrderedDict()\n",
    "    for prob, topic in flat_list:\n",
    "        d.setdefault(topic, []).append(prob)\n",
    "\n",
    "    d = [(sum(v) / len(v), k) for k, v in d.items()]\n",
    "    \n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walmart model applied to Walmart documents\n",
      "[(0.31775094853448016, 0), (0.6921117425738991, 1), (0.4539582208888803, 2), (0.587554996434067, 3), (0.5921518906521109, 4), (0.3510176306590438, 5), (0.23573007574304938, 6), (0.03703961428254843, 7), (0.4609524873235533, 8), (0.587985881648603, 9)]\n",
      "\n",
      "Walmart model applied to Amazon documents\n",
      "[(0.08312106970697641, 0), (0.2005708650976885, 1), (0.41248021469348006, 2), (0.23937216152747473, 3), (0.2566982574125593, 4), (0.019593778997659683, 5), (0.050565017946064474, 6), (0.10522330237122682, 7), (0.14487355951111294, 8), (0.08065227537216353, 9)]\n",
      "\n",
      "Walmart model applied to Costco documents\n",
      "[(0.06984300389885903, 0), (0.24439339786065076, 1), (0.34193369271760843, 2), (0.3664987232941802, 4), (0.02445489577949047, 6), (0.25081570621293325, 8), (0.09493264649063349, 9)]\n",
      "\n",
      "Walmart model applied to Target documents\n",
      "[(0.02922035888251331, 0), (0.2843753030716345, 1), (0.4404372685784652, 2), (0.025487614950786035, 3), (0.2778961125406481, 4), (0.05751107766159943, 6), (0.06956358635354609, 7), (0.23074507399267236, 8), (0.13466546630317514, 9)]\n",
      "\n",
      "Walmart model applied to Kroger documents\n",
      "[(0.06468194164335728, 0), (0.2873687411924558, 1), (0.3408226478923723, 2), (0.039916519075632095, 3), (0.2956157100250882, 4), (0.22541603120043874, 6), (0.08222174706558387, 7), (0.22766488746807562, 8), (0.07917437701105652, 9)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "for index, pair in enumerate(names_prod[:5]):\n",
    "    model_txt  = pair[0][0]\n",
    "    corpus_txt = pair[1][0]\n",
    "    \n",
    "    model  = pair[0][2]\n",
    "    corpus = pair[1][1]\n",
    "    \n",
    "    print(f\"{model_txt} model applied to {corpus_txt} documents\")\n",
    "    tag = [model.get_document_topics(item) for item in corpus]\n",
    "    tag = [tup[::-1] for tup in flatten(tag)]\n",
    "    \n",
    "    topic_avg = average_topic(tag)\n",
    "    topic_avg = sorted(topic_avg, key = lambda x: x[1])\n",
    "    \n",
    "    print(topic_avg)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
